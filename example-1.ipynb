{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Physics Analysis with AI Techniques\n",
    "\n",
    "This notebook demonstrates utilizing AI techniques to analyze large datasets from particle physics experiments to identify patterns and new particles. We'll use machine learning techniques such as clustering and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset representing particle physics data\n",
    "def generate_synthetic_data(n_samples=10000):\n",
    "    np.random.seed(42)\n",
    "    # Features: energy, momentum, mass, charge, etc.\n",
    "    data = {\n",
    "        'energy': np.random.normal(loc=50, scale=10, size=n_samples),\n",
    "        'momentum_x': np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        'momentum_y': np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        'momentum_z': np.random.normal(loc=0, scale=1, size=n_samples),\n",
    "        'mass': np.random.exponential(scale=1, size=n_samples),\n",
    "        'charge': np.random.choice([-1, 0, 1], size=n_samples)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Simulate labels for known particles and an unknown particle\n",
    "    known_particles = np.random.choice([0, 1], size=n_samples, p=[0.9, 0.1])\n",
    "    unknown_particle = np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01])\n",
    "    df['label'] = known_particles + unknown_particle\n",
    "    df['label'] = df['label'].apply(lambda x: 2 if x > 1 else x)  # 2 represents the unknown particle\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = generate_synthetic_data()\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Visualization of the data using PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train, palette='viridis', s=50)\n",
    "plt.title('PCA of Particle Physics Data')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Particle Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using KMeans to identify potential new particles\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "y_train_clusters = kmeans.fit_predict(X_train_pca)\n",
    "y_test_clusters = kmeans.predict(X_test_pca)\n",
    "\n",
    "# Visualization of the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y_train_clusters, palette='viridis', s=50)\n",
    "plt.title('KMeans Clustering of Particle Physics Data')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification using Random Forest to identify known and unknown particles\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred, target_names=['Known Particle 1', 'Known Particle 2', 'Unknown Particle']))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "feature_importances.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "plt.title('Feature Importances in Particle Classification')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
